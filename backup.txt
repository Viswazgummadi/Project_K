import gradio as gr
import os
import PyPDF2

from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.postprocessor import SimilarityPostprocessor
from datasets import DatasetDict, Dataset
import re
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

def respond(files, question):
    os.makedirs("pdf", exist_ok=True)

    documents = []

    for i, file in enumerate(files):
        if file.endswith('.pdf'):
            pdf_file_obj = open(file, 'rb')
            pdf_reader = PyPDF2.PdfFileReader(pdf_file_obj)
            text = ''
            for page_num in range(pdf_reader.numPages):
                page_obj = pdf_reader.getPage(page_num)
                text += page_obj.extractText()
            documents.append(text)
        else:
            with open(file, 'r') as f:
                documents.append(f.read())

    # import any embedding model on HF hub (https://huggingface.co/spaces/mteb/leaderboard)
    Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
    # Settings.embed_model = HuggingFaceEmbedding(model_name="thenlper/gte-large") # alternative model

    Settings.llm = None
    Settings.chunk_size = 256
    Settings.chunk_overlap = 25

    # some ad hoc document refinement
    for doc in documents:
        if "Member-only story" in doc:
            documents.remove(doc)
            continue

        if "The Data Entrepreneurs" in doc:
            documents.remove(doc)

        if " min read" in doc:
            documents.remove(doc)

    # store docs into vector DB
    index = VectorStoreIndex.from_documents(documents)
    # set number of docs to retrieve
    top_k = 3

    # configure retriever
    retriever = VectorIndexRetriever(
        index=index,
        similarity_top_k=top_k,
    )
    # assemble query engine
    query_engine = RetrieverQueryEngine(
        retriever=retriever,
        node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],
    )

    # create a list to store the mapping of questions to their respective contexts
    question_context_pairs = []

    # iterate over each question and query the engine
    response = query_engine.query(question)

    # create a combined context from the top 3 chunks
    context = "Context:\n"
    for j in range(top_k):
        context = context + response.source_nodes[j].text + "\n\n"

    # store the mapping of the question to its respective context
    question_context_pairs.append({"question": question, "context": context})

    # create a dictionary with two lists: questions and contexts
    data = {
        "question": [pair["question"] for pair in question_context_pairs],
        "context": [pair["context"] for pair in question_context_pairs]
    }

    # create a Hugging Face Dataset from the dictionary
    dataset = Dataset.from_dict(data)

    from transformers import AutoTokenizer, AutoModelForCausalLM

    # model_id="mistralai/Mistral-7B-Instruct-v0.2"
    model_id="Dobby091/KOKO"
    # Load the tokenizer and the model
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        cache_dir = "/mnt/data1/backup/viswaz/Project_K/huggingface_cache/",
    )

    def generate_response(prompt, model):
      encoded_input = tokenizer(prompt,  return_tensors="pt", add_special_tokens=True)
      model_inputs = encoded_input.to('cuda')

      generated_ids = model.generate(**model_inputs,
                                     max_new_tokens=150,
                                     do_sample=True,
                                     pad_token_id=tokenizer.eos_token_id)

      return generated_ids

    intstructions_string = f""" you are a textbot that helps in finding answers to questions in the research papers, blogs,pdf's or any text context.
    ,make your answers more meaningful and short"

    please answer the following question
    """
    prompt_template_w_context = lambda context, question: f'''[INST] {intstructions_string}

    {context}

    Please answer to the following question. Use the context above if it is helpful.

    {question}

    [/INST]'''

    prompt = prompt_template_w_context(context, question)

    outputs = generate_response(prompt, model)

    # Decode the generated IDs
    decoded_output = tokenizer.batch_decode(outputs)[0]

    # Extract texts between [/INST] and </s> tags
    inst_eos_texts = re.findall(r'\[\/INST\](.*?)\<\/s\>', decoded_output, re.DOTALL)

    return inst_eos_texts[0].strip() if inst_eos_texts else ""

inputs = [
    gr.File(type="filepath", label="Upload PDF/Text Files",file_count="multiple"),
    gr.Text(label="Enter your question here")
]

output = gr.Text()

interface = gr.Interface(respond, inputs=inputs, outputs=output, title="Question Answering System")

if __name__ == "__main__":
    interface.launch()
