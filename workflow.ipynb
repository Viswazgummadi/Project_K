{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "with open('questions.txt', 'r') as file:\n",
    "    # Read all lines in the file\n",
    "    questions = file.readlines()\n",
    "\n",
    "# Now you can access each question by its index, like in a list\n",
    "# for i in range(len(questions)):\n",
    "#     print(f\"Question[{i}]: {questions[i].strip()}\")\n",
    "\n",
    "# print(questions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# import any embedding model on HF hub (https://huggingface.co/spaces/mteb/leaderboard)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"thenlper/gte-large\") # alternative model\n",
    "\n",
    "Settings.llm = None\n",
    "Settings.chunk_size = 256\n",
    "Settings.chunk_overlap = 25\n",
    "\n",
    "# articles available here:  {add GitHub repo}\n",
    "documents = SimpleDirectoryReader(\"pdf\").load_data()\n",
    "\n",
    "# some ad hoc document refinement\n",
    "for doc in documents:\n",
    "    if \"Member-only story\" in doc.text:\n",
    "        documents.remove(doc)\n",
    "        continue\n",
    "\n",
    "    if \"The Data Entrepreneurs\" in doc.text:\n",
    "        documents.remove(doc)\n",
    "\n",
    "    if \" min read\" in doc.text:\n",
    "        documents.remove(doc)\n",
    "\n",
    "# store docs into vector DB\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "# set number of docs to retrieve\n",
    "top_k = 3\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=top_k,\n",
    ")\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
    ")\n",
    "\n",
    "# load questions\n",
    "with open('questions.txt', 'r') as file:\n",
    "    questions = file.readlines()\n",
    "\n",
    "# strip newline characters from questions\n",
    "questions = [q.strip() for q in questions]\n",
    "\n",
    "# create a list to store the mapping of questions to their respective contexts\n",
    "question_context_pairs = []\n",
    "\n",
    "# iterate over each question and query the engine\n",
    "for i, question in enumerate(questions):\n",
    "    response = query_engine.query(question)\n",
    "\n",
    "    # create a combined context from the top 3 chunks\n",
    "    context = \"Context:\\n\"\n",
    "    for j in range(top_k):\n",
    "        context = context + response.source_nodes[j].text + \"\\n\\n\"\n",
    "\n",
    "    # store the mapping of the question to its respective context\n",
    "    question_context_pairs.append({\"question\": question, \"context\": context})\n",
    "\n",
    "# create a dictionary with two lists: questions and contexts\n",
    "data = {\n",
    "    \"question\": [pair[\"question\"] for pair in question_context_pairs],\n",
    "    \"context\": [pair[\"context\"] for pair in question_context_pairs]\n",
    "}\n",
    "\n",
    "# create a Hugging Face Dataset from the dictionary\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# print the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa7f4f93e3c4addb70b0d310e040dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir = \"/mnt/data1/backup/viswaz/Project_K/huggingface_cache/\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# def generate_response(prompt, model):\n",
    "#   encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "#   model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "#   generated_ids = model.generate(**model_inputs,\n",
    "#                                  max_new_tokens=150,\n",
    "#                                  do_sample=True,\n",
    "#                                  pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#   decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "#   # Split the generated text into prompt and answer\n",
    "#   split_index = decoded_output[0].find('answer:')\n",
    "#   if split_index == -1:\n",
    "#     split_index = decoded_output[0].find('Answer:')\n",
    "#   decoded_output = [s.strip(\"'\") for s in decoded_output]\n",
    "#   decoded_output = [s.replace('</s>', '') for s in decoded_output]\n",
    "#   answer = decoded_output[0][split_index+len('answer:'):].strip()\n",
    "#   answer = answer.replace('[INST]'or '[ANS]' , '').replace('[/INST]' or '[/ANS]' or '[//INST]', '')\n",
    "#   answer = re.sub(r'\\[.*?\\]', '', answer)\n",
    "\n",
    "#   # Stop decoding when it encounters a ### token\n",
    "#   stop_index = answer.find('###')\n",
    "#   if stop_index != -1:\n",
    "#     answer = answer[:stop_index].strip()\n",
    "\n",
    "#   return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize an empty list to store the answers\n",
    "# answers = []\n",
    "\n",
    "# # Iterate over each question in the dataset\n",
    "# for i in range(len(dataset)):\n",
    "#     # Get the question and the context\n",
    "#     question = dataset[i][\"question\"]\n",
    "#     context = dataset[i][\"context\"]\n",
    "\n",
    "#     # Define the prompt as the concatenation of the context and the question\n",
    "#     prompt = prompt_template_w_context(context, question)\n",
    "\n",
    "#     outputs = generate_response(prompt, model)\n",
    "\n",
    "#     print(tokenizer.batch_decode(outputs)[0])\n",
    "\n",
    "# # # Print the answers\n",
    "# # for i, answer in enumerate(answers):\n",
    "# #     print(f\"Question {i+1}: {dataset[i]['question']}\")\n",
    "# #     print(\"\\n\\n\\n\")\n",
    "# #     print(f\"Answer: {answer}\\n\")\n",
    "# #     print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d8fc5abc5a436abdae5f9c60396c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# import any embedding model on HF hub (https://huggingface.co/spaces/mteb/leaderboard)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"thenlper/gte-large\") # alternative model\n",
    "\n",
    "Settings.llm = None\n",
    "Settings.chunk_size = 256\n",
    "Settings.chunk_overlap = 25\n",
    "\n",
    "# articles available here:  {add GitHub repo}\n",
    "documents = SimpleDirectoryReader(\"pdf\").load_data()\n",
    "\n",
    "# some ad hoc document refinement\n",
    "for doc in documents:\n",
    "    if \"Member-only story\" in doc.text:\n",
    "        documents.remove(doc)\n",
    "        continue\n",
    "\n",
    "    if \"The Data Entrepreneurs\" in doc.text:\n",
    "        documents.remove(doc)\n",
    "\n",
    "    if \" min read\" in doc.text:\n",
    "        documents.remove(doc)\n",
    "\n",
    "# store docs into vector DB\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "# set number of docs to retrieve\n",
    "top_k = 3\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=top_k,\n",
    ")\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
    ")\n",
    "\n",
    "# load questions\n",
    "with open('questions.txt', 'r') as file:\n",
    "    questions = file.readlines()\n",
    "\n",
    "# strip newline characters from questions\n",
    "questions = [q.strip() for q in questions]\n",
    "\n",
    "# create a list to store the mapping of questions to their respective contexts\n",
    "question_context_pairs = []\n",
    "\n",
    "# iterate over each question and query the engine\n",
    "for i, question in enumerate(questions):\n",
    "    response = query_engine.query(question)\n",
    "\n",
    "    # create a combined context from the top 3 chunks\n",
    "    context = \"Context:\\n\"\n",
    "    for j in range(top_k):\n",
    "        context = context + response.source_nodes[j].text + \"\\n\\n\"\n",
    "\n",
    "    # store the mapping of the question to its respective context\n",
    "    question_context_pairs.append({\"question\": question, \"context\": context})\n",
    "\n",
    "# create a dictionary with two lists: questions and contexts\n",
    "data = {\n",
    "    \"question\": [pair[\"question\"] for pair in question_context_pairs],\n",
    "    \"context\": [pair[\"context\"] for pair in question_context_pairs]\n",
    "}\n",
    "\n",
    "# create a Hugging Face Dataset from the dictionary\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir = \"/mnt/data1/backup/viswaz/Project_K/huggingface_cache/\",\n",
    ")\n",
    "\n",
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs,\n",
    "                                 max_new_tokens=150,\n",
    "                                 do_sample=True,\n",
    "                                 pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  return generated_ids\n",
    "\n",
    "# prompt (no context)\n",
    "intstructions_string = f\"\"\" you are a textbot that helps in finding answers to questions in the research papers, blogs,pdf's or any text context.\n",
    ",make your answers more meaningful and short,end all responses with a signature after answer \"-yourbot\"\n",
    "\n",
    "please answer the following question\n",
    "\"\"\"\n",
    "prompt_template_w_context = lambda context, question: f'''[INST] {intstructions_string}\n",
    "\n",
    "{context}\n",
    "\n",
    "Please answer to the following question. Use the context above if it is helpful.\n",
    "\n",
    "{question}\n",
    "\n",
    "[/INST]'''\n",
    "# Initialize an empty list to store the answers\n",
    "answers = []\n",
    "\n",
    "# Iterate over each question in the dataset\n",
    "for i in range(len(dataset)):\n",
    "    # Get the question and the context\n",
    "    question = dataset[i][\"question\"]\n",
    "    context = dataset[i][\"context\"]\n",
    "\n",
    "    # Define the prompt as the concatenation of the context and the question\n",
    "    prompt = prompt_template_w_context(context, question)\n",
    "\n",
    "    outputs = generate_response(prompt, model)\n",
    "\n",
    "    # Decode the generated IDs and store the answer\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    answers.append(decoded_output)\n",
    "\n",
    "# Print the answers\n",
    "for i, answer in enumerate(answers):\n",
    "    print(f\"Question {i+1}: {dataset[i]['question']}\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# import any embedding model on HF hub (https://huggingface.co/spaces/mteb/leaderboard)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"thenlper/gte-large\") # alternative model\n",
    "\n",
    "Settings.llm = None\n",
    "Settings.chunk_size = 256\n",
    "Settings.chunk_overlap = 25\n",
    "\n",
    "# articles available here:  {add GitHub repo}\n",
    "documents = SimpleDirectoryReader(\"pdf\").load_data()\n",
    "\n",
    "# some ad hoc document refinement\n",
    "for doc in documents:\n",
    "    if \"Member-only story\" in doc.text:\n",
    "        documents.remove(doc)\n",
    "        continue\n",
    "\n",
    "    if \"The Data Entrepreneurs\" in doc.text:\n",
    "        documents.remove(doc)\n",
    "\n",
    "    if \" min read\" in doc.text:\n",
    "        documents.remove(doc)\n",
    "\n",
    "# store docs into vector DB\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "# set number of docs to retrieve\n",
    "top_k = 3\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=top_k,\n",
    ")\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
    ")\n",
    "\n",
    "# load questions\n",
    "with open('questions.txt', 'r') as file:\n",
    "    questions = file.readlines()\n",
    "\n",
    "# strip newline characters from questions\n",
    "questions = [q.strip() for q in questions]\n",
    "\n",
    "# create a list to store the mapping of questions to their respective contexts\n",
    "question_context_pairs = []\n",
    "\n",
    "# iterate over each question and query the engine\n",
    "for i, question in enumerate(questions):\n",
    "    response = query_engine.query(question)\n",
    "\n",
    "    # create a combined context from the top 3 chunks\n",
    "    context = \"Context:\\n\"\n",
    "    for j in range(top_k):\n",
    "        context = context + response.source_nodes[j].text + \"\\n\\n\"\n",
    "\n",
    "    # store the mapping of the question to its respective context\n",
    "    question_context_pairs.append({\"question\": question, \"context\": context})\n",
    "\n",
    "# create a dictionary with two lists: questions and contexts\n",
    "data = {\n",
    "    \"question\": [pair[\"question\"] for pair in question_context_pairs],\n",
    "    \"context\": [pair[\"context\"] for pair in question_context_pairs]\n",
    "}\n",
    "\n",
    "# create a Hugging Face Dataset from the dictionary\n",
    "dataset = Dataset.from_dict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330f549f699440a2b7fd230cfecf6a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir = \"/mnt/data1/backup/viswaz/Project_K/huggingface_cache/\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs,\n",
    "                                 max_new_tokens=150,\n",
    "                                 do_sample=True,\n",
    "                                 pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  return generated_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt (no context)\n",
    "intstructions_string = f\"\"\" you are a textbot that helps in finding answers to questions in the research papers, blogs,pdf's or any text context.\n",
    ",make your answers more meaningful and short\"\n",
    "\n",
    "please answer the following question\n",
    "\"\"\"\n",
    "prompt_template_w_context = lambda context, question: f'''[INST] {intstructions_string}\n",
    "\n",
    "{context}\n",
    "\n",
    "Please answer to the following question. Use the context above if it is helpful.\n",
    "\n",
    "{question}\n",
    "\n",
    "[/INST]'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Initialize an empty list to store the answers\n",
    "answers = []\n",
    "texts_between_inst_and_eos = []\n",
    "# Iterate over each question in the dataset\n",
    "for i in range(len(dataset)):\n",
    "    # Get the question and the context\n",
    "    question = dataset[i][\"question\"]\n",
    "    context = dataset[i][\"context\"]\n",
    "\n",
    "    # Define the prompt as the concatenation of the context and the question\n",
    "    prompt = prompt_template_w_context(context, question)\n",
    "\n",
    "    outputs = generate_response(prompt, model)\n",
    "\n",
    "    # Decode the generated IDs and store the answer\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    answers.append(decoded_output)\n",
    "    \n",
    "    inst_eos_texts = re.findall(r'\\[\\/INST\\](.*?)\\<\\/s\\>', decoded_output, re.DOTALL)\n",
    "    texts_between_inst_and_eos.extend(inst_eos_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A separate standard (IS : 228-1959*) was retained for the analysis of pig iron and cast iron because a comprehensive series on chemical analysis of steels including high alloy steels was published instead (IS : 228, Parts 1 to 13), and a separate standard for the analysis of pig iron and cast iron had not been published yet.\n",
      "The reproducibility for nickel content between 0.5 to 5 percent is f 0.050 percent.\n",
      "The recommended concentrations of hydrochloric acid in the context are 1 : 19 and 1 : 3. The first concentration is used for diluting a concentrated hydrochloric acid solution, and the second concentration is used for the determination process in steps 6.1.2 and 6.1.3.\n"
     ]
    }
   ],
   "source": [
    "for text in texts_between_inst_and_eos:\n",
    "    print(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Print the answers\n",
    "# for i, answer in enumerate(answers):\n",
    "#     # print(f\"Question {i+1}: {dataset[i]['question']}\")\n",
    "#     print(\"\\n\\n\\n\")\n",
    "#     print(f\"{answer}\\n\")\n",
    "#     print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A separate standard (IS: 228-1959*) was retained for the analysis of pig iron and cast iron because a comprehensive series on chemical analysis of steels, including high alloy steels, was being published as IS: 228 (Parts 1 to 13), and a separate standard for pig iron and cast iron was not yet published.\n",
      "The reproducibility for nickel content between 0.5 to 5 percent is 0.050 percent according to IS 228 (Part 5) - 1987.\n",
      "The recommended concentration of hydrochloric acid for the given procedure is 1 : 19 (v/v), meaning that 1 part of concentrated hydrochloric acid should be diluted with 19 parts of water.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Initialize an empty list to store the texts between [/INST] and </s> tags\n",
    "texts_between_inst_and_eos = []\n",
    "\n",
    "# Iterate over each question in the dataset\n",
    "for i in range(len(dataset)):\n",
    "    # Get the question and the context\n",
    "    question = dataset[i][\"question\"]\n",
    "    context = dataset[i][\"context\"]\n",
    "\n",
    "    # Define the prompt as the concatenation of the context and the question\n",
    "    prompt = prompt_template_w_context(context, question)\n",
    "\n",
    "    outputs = generate_response(prompt, model)\n",
    "\n",
    "    # Decode the generated IDs\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "    # Extract texts between [/INST] and </s> tags\n",
    "    inst_eos_texts = re.findall(r'\\[\\/INST\\](.*?)\\<\\/s\\>', decoded_output, re.DOTALL)\n",
    "    texts_between_inst_and_eos.extend(inst_eos_texts)\n",
    "\n",
    "# Print the texts between [/INST] and </s> tags\n",
    "# print(\"Texts between [/INST] and </s> tags:\")\n",
    "for text in texts_between_inst_and_eos:\n",
    "    print(text.strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
