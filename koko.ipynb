{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers bitsandbytes accelerate datasets peft trl\n",
    "# !pip install transformers trl accelerate torch bitsandbytes peft datasets\n",
    "# !pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc176e59c8d44e5ab299351304785d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/488 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ed68e58fe34f5d9ad44e6653eaca6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/414k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb951f6173bd44cbaba71bb455005bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08c55a0c2c547a09482c392ac123c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b5ad0476594c43b07a3e5d8db67a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/222 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the instruct_tune_dataset dataset\n",
    "dataset = load_dataset(\"Dobby091/koko\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pdf_filename', 'context', 'question', 'answer'],\n",
       "        num_rows: 884\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pdf_filename', 'context', 'question', 'answer'],\n",
       "        num_rows: 222\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(dataset[\"test\"][0])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 15\n",
      "Number of test samples: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pdf_filename', 'context', 'question', 'answer'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pdf_filename', 'context', 'question', 'answer'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from datasets import Dataset, DatasetDict\n",
    "\n",
    "# # Select the first 1000 samples in the train set and the first 200 samples in the test set\n",
    "# train_dataset = dataset['train'].select(range(15))\n",
    "# test_dataset = dataset['test'].select(range(2))\n",
    "\n",
    "# # Create a new `DatasetDict` to store the selected samples\n",
    "# selected_dataset_dict = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "\n",
    "# # Print the number of samples in each split\n",
    "# print(f\"Number of train samples: {len(train_dataset)}\")\n",
    "# print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "\n",
    "# selected_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now lets keep selected dataset as total dataset because of low data\n",
    "selected_dataset_dict = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "# !pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "# import any embedding model on HF hub (https://huggingface.co/spaces/mteb/leaderboard)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"thenlper/gte-large\") # alternative model\n",
    "\n",
    "Settings.llm = None\n",
    "Settings.chunk_size = 256\n",
    "Settings.chunk_overlap = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\"CustomDocument\" object has no field \"doc_id\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m# Generate a unique identifier for the document\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         doc_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4())\n\u001b[0;32m---> 30\u001b[0m         document \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpdf_filename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunk_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdoc_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_id\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m         documents\u001b[38;5;241m.\u001b[39mappend(document)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Rest of the code...\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m, in \u001b[0;36mCustomDocument.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_id\u001b[49m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/pydantic/v1/main.py:357\u001b[0m, in \u001b[0;36mBaseModel.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m object_setattr(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mextra \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Extra\u001b[38;5;241m.\u001b[39mallow \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__fields__:\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m object has no field \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mallow_mutation \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mfrozen:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is immutable and does not support item assignment\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: \"CustomDocument\" object has no field \"doc_id\""
     ]
    }
   ],
   "source": [
    "\n",
    "# articles available here:  {add GitHub repo}\n",
    "documents = SimpleDirectoryReader(\"pdf\").load_data()\n",
    "\n",
    "# some ad hoc document refinement\n",
    "print(len(documents))\n",
    "for doc in documents:\n",
    "    if \"Member-only story\" in doc.text:\n",
    "        documents.remove(doc)\n",
    "        continue\n",
    "\n",
    "    if \"The Data Entrepreneurs\" in doc.text:\n",
    "        documents.remove(doc)\n",
    "\n",
    "    if \" min read\" in doc.text:\n",
    "        documents.remove(doc)\n",
    "\n",
    "print(len(documents))\n",
    "\n",
    "# store docs into vector DB\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "# set number of docs to retreive\n",
    "top_k = 3\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=top_k,\n",
    ")\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
    ")\n",
    "# query documents\n",
    "query = \"When was this standard adopted?\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# reformat response\n",
    "context = \"Context:\\n\"\n",
    "for i in range(top_k):\n",
    "    context = context + response.source_nodes[i].text + \"\\n\\n\"\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pdf_filename', 'question', 'answer'],\n",
       "        num_rows: 72\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pdf_filename', 'question', 'answer'],\n",
       "        num_rows: 18\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "<s>[INST]###Instruction:\n",
      "below context is from 228_4.pdf, answer the following questions based on the context given \n",
      "\n",
      "\n",
      "###pdf_filename:\n",
      "228_4.pdf\n",
      "\n",
      "###context:\n",
      "Context:\n",
      "IS : ‘228 ( Part 4 ) - 1987 \n",
      "Indian Standard \n",
      "METHODS FOR \n",
      "CHEMICAL ANALYSIS OF STEELS \n",
      "PART 4 DETERYlNATlON OF TOTAL CARBON \n",
      "BY GARVIMETRIC METHOD \n",
      "(FOR CARBON > O-1 PERCENT ) \n",
      "( Third Revision ) \n",
      "0. FOREWORD \n",
      "0.1 This Indian Standard ( Part 4 ) (Third R cvision ) was adopted I)y \n",
      "the Indian Standards Institution on 16 January 1987, afirr the draft \n",
      "finalized by the Methods of Chemical Analysis of Pcrrous Metals \n",
      "Sectional Committre had been approved by the Structural and Mct;~ls \n",
      "Division Council. \n",
      "0.2 IS : 228, which was issued as a tentative standard in 1952 and \n",
      "revised in 1959, covered the chemical analysis of pig iron, cast iron and \n",
      "plain carbon and low alloy steels.\n",
      "\n",
      "IS t \n",
      "Indian Standard 228 ( Part 5 ) - 1987 \n",
      "METHODS FOR \n",
      "CHEMICAL ANALYSIS OF STEELS \n",
      "PART 6 DETERMINATION OF NICKEL BY \n",
      "DIMETHYLGLYOXIME ( GRAVIMETRIC ) METHOD \n",
      "( FOR NICKEL > 0’1 PERCENT) \n",
      "( Third Revision ) \n",
      "0. FOREWORD \n",
      "0.1 This Indian Standard ( Part 5 ) ( Third Revision ) was adopted by \n",
      "the Indian Standards Institution on 16 January 1987, after the draft \n",
      "finalized by the Methods of Chemical Analysis of Ferrous Metals \n",
      "Sectional Committee had been approved by the Strtictural and Metals \n",
      "Division Council. \n",
      "0.2 IS : 228, which was issued as a tentative standard in 1952 and \n",
      "revised in 1959, covered the chemial analysis of pig iron, cast iron and \n",
      "plain carbon and low alloy steels.\n",
      "\n",
      "Accordingly, revision of IS : 228 was \n",
      "taken-up again and new series on methods of chemical analysis of \n",
      "steels including high alloy steels was published in various parts as \n",
      "IS : 228 ( Parts 1 to 13 ) ( see Appendix A ) covering separate method \n",
      "of analysis for each constituent in steels. However, IS : 228-1959* \n",
      "version has been retained for the analysis of pig iron and cast iron \n",
      "till a separate standard for analysis of pig iron and cast iron is \n",
      "published. \n",
      "0.2.1 This revision of IS : 228 (Part 6 )-1974t has been undertaken \n",
      "on the basis of experience gained during the implementation of the \n",
      "standard by the manufacturers and testing laboratories. \n",
      "0.3 In this revision, major modifications are: \n",
      "a) scope of the method has been modified by lowering the limit \n",
      "for determination of chromium from 0’5 to 0’1 percent; \n",
      "-.\n",
      "\n",
      "\n",
      "\n",
      "###question:\n",
      "2. When was this standard adopted?[/INST]\n",
      "\n",
      "###answer:\n",
      "1987-01-16 00:00:00</s>\n",
      "---------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>[INST]###Instruction:\\nbelow context is from 228_4.pdf, answer the following questions based on the context given \\n\\n\\n###pdf_filename:\\n228_4.pdf\\n\\n###context:\\nContext:\\nIS : ‘228 ( Part 4 ) - 1987 \\nIndian Standard \\nMETHODS FOR \\nCHEMICAL ANALYSIS OF STEELS \\nPART 4 DETERYlNATlON OF TOTAL CARBON \\nBY GARVIMETRIC METHOD \\n(FOR CARBON > O-1 PERCENT ) \\n( Third Revision ) \\n0. FOREWORD \\n0.1 This Indian Standard ( Part 4 ) (Third R cvision ) was adopted I)y \\nthe Indian Standards Institution on 16 January 1987, afirr the draft \\nfinalized by the Methods of Chemical Analysis of Pcrrous Metals \\nSectional Committre had been approved by the Structural and Mct;~ls \\nDivision Council. \\n0.2 IS : 228, which was issued as a tentative standard in 1952 and \\nrevised in 1959, covered the chemical analysis of pig iron, cast iron and \\nplain carbon and low alloy steels.\\n\\nIS t \\nIndian Standard 228 ( Part 5 ) - 1987 \\nMETHODS FOR \\nCHEMICAL ANALYSIS OF STEELS \\nPART 6 DETERMINATION OF NICKEL BY \\nDIMETHYLGLYOXIME ( GRAVIMETRIC ) METHOD \\n( FOR NICKEL > 0’1 PERCENT) \\n( Third Revision ) \\n0. FOREWORD \\n0.1 This Indian Standard ( Part 5 ) ( Third Revision ) was adopted by \\nthe Indian Standards Institution on 16 January 1987, after the draft \\nfinalized by the Methods of Chemical Analysis of Ferrous Metals \\nSectional Committee had been approved by the Strtictural and Metals \\nDivision Council. \\n0.2 IS : 228, which was issued as a tentative standard in 1952 and \\nrevised in 1959, covered the chemial analysis of pig iron, cast iron and \\nplain carbon and low alloy steels.\\n\\nAccordingly, revision of IS : 228 was \\ntaken-up again and new series on methods of chemical analysis of \\nsteels including high alloy steels was published in various parts as \\nIS : 228 ( Parts 1 to 13 ) ( see Appendix A ) covering separate method \\nof analysis for each constituent in steels. However, IS : 228-1959* \\nversion has been retained for the analysis of pig iron and cast iron \\ntill a separate standard for analysis of pig iron and cast iron is \\npublished. \\n0.2.1 This revision of IS : 228 (Part 6 )-1974t has been undertaken \\non the basis of experience gained during the implementation of the \\nstandard by the manufacturers and testing laboratories. \\n0.3 In this revision, major modifications are: \\na) scope of the method has been modified by lowering the limit \\nfor determination of chromium from 0’5 to 0’1 percent; \\n-.\\n\\n\\n\\n###question:\\n2. When was this standard adopted?[/INST]\\n\\n###answer:\\n1987-01-16 00:00:00</s>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_prompt(sample):\n",
    "    bos_token = \"<s>\"\n",
    "    base_prompt1 = \"below context is from \"\n",
    "    base_prompt2 = \", answer the following questions based on the context given \\n\"\n",
    "    document = sample['pdf_filename']\n",
    "    # context = sample['context']\n",
    "    answer = sample['answer']\n",
    "    question = sample['question']\n",
    "    eos_token = \"</s>\"\n",
    "    full_prompt = \"\"\n",
    "    full_prompt += bos_token\n",
    "    full_prompt += \"[INST]\"\n",
    "    full_prompt += \"###Instruction:\\n\"\n",
    "    full_prompt += base_prompt1\n",
    "    full_prompt += document\n",
    "    full_prompt += base_prompt2\n",
    "    # full_prompt += \"[INST]\"\n",
    "    full_prompt += \"\\n\\n###pdf_filename:\\n\" + document\n",
    "    full_prompt += \"\\n\\n###context:\\n\" + context\n",
    "    full_prompt += \"\\n\\n###question:\\n\" + question\n",
    "    full_prompt += \"[/INST]\"\n",
    "    full_prompt += \"\\n\\n###answer:\\n\" + answer\n",
    "    full_prompt += eos_token\n",
    "    print(\"------------------\")\n",
    "    print(full_prompt)\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "    return full_prompt\n",
    "\n",
    "create_prompt(selected_dataset_dict[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "# model_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_id=\"mistralai/Mistral-7B-v0.1\"\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad53129d071548248d8dabd0fe737277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "os.environ['HF_HOME'] = '/mnt/data1/backup/viswaz/Project_K/huggingface_cache/'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=nf4_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir = \"/mnt/data1/backup/viswaz/Project_K/huggingface_cache/\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_size = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs,\n",
    "                                 max_new_tokens=512,\n",
    "                                 do_sample=True,\n",
    "                                 pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "  return decoded_output[0].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> This standard was adopted on 16 January 1987. It is a third revision of Indian Standard 228, Part 4 and Part 5, which were first issued as tentative standards in 1952 and revised in 1959. The revision of IS 228 was taken up again and a new series on methods of chemical analysis of steels was published in various parts from 1987 to 1994, including IS 228-Part 6 for determination of nickel by dimethylglyoxime gravimetric method for nickel > 0.1 percent.</s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "[INST]\n",
    "###Instruction:\n",
    "below context is from 228_4.pdf, answer the following questions based on the context given \n",
    "\n",
    "\n",
    "###pdf_filename:\n",
    "228_4.pdf\n",
    "\n",
    "###context:\n",
    "Context:\n",
    "IS : ‘228 ( Part 4 ) - 1987 \n",
    "Indian Standard \n",
    "METHODS FOR \n",
    "CHEMICAL ANALYSIS OF STEELS \n",
    "PART 4 DETERYlNATlON OF TOTAL CARBON \n",
    "BY GARVIMETRIC METHOD \n",
    "(FOR CARBON > O-1 PERCENT ) \n",
    "( Third Revision ) \n",
    "0. FOREWORD \n",
    "0.1 This Indian Standard ( Part 4 ) (Third R cvision ) was adopted I)y \n",
    "the Indian Standards Institution on 16 January 1987, afirr the draft \n",
    "finalized by the Methods of Chemical Analysis of Pcrrous Metals \n",
    "Sectional Committre had been approved by the Structural and Mct;~ls \n",
    "Division Council. \n",
    "0.2 IS : 228, which was issued as a tentative standard in 1952 and \n",
    "revised in 1959, covered the chemical analysis of pig iron, cast iron and \n",
    "plain carbon and low alloy steels.\n",
    "\n",
    "IS t \n",
    "Indian Standard 228 ( Part 5 ) - 1987 \n",
    "METHODS FOR \n",
    "CHEMICAL ANALYSIS OF STEELS \n",
    "PART 6 DETERMINATION OF NICKEL BY \n",
    "DIMETHYLGLYOXIME ( GRAVIMETRIC ) METHOD \n",
    "( FOR NICKEL > 0’1 PERCENT) \n",
    "( Third Revision ) \n",
    "0. FOREWORD \n",
    "0.1 This Indian Standard ( Part 5 ) ( Third Revision ) was adopted by \n",
    "the Indian Standards Institution on 16 January 1987, after the draft \n",
    "finalized by the Methods of Chemical Analysis of Ferrous Metals \n",
    "Sectional Committee had been approved by the Strtictural and Metals \n",
    "Division Council. \n",
    "0.2 IS : 228, which was issued as a tentative standard in 1952 and \n",
    "revised in 1959, covered the chemial analysis of pig iron, cast iron and \n",
    "plain carbon and low alloy steels.\n",
    "\n",
    "Accordingly, revision of IS : 228 was \n",
    "taken-up again and new series on methods of chemical analysis of \n",
    "steels including high alloy steels was published in various parts as \n",
    "IS : 228 ( Parts 1 to 13 ) ( see Appendix A ) covering separate method \n",
    "of analysis for each constituent in steels. However, IS : 228-1959* \n",
    "version has been retained for the analysis of pig iron and cast iron \n",
    "till a separate standard for analysis of pig iron and cast iron is \n",
    "published. \n",
    "0.2.1 This revision of IS : 228 (Part 6 )-1974t has been undertaken \n",
    "on the basis of experience gained during the implementation of the \n",
    "standard by the manufacturers and testing laboratories. \n",
    "0.3 In this revision, major modifications are: \n",
    "a) scope of the method has been modified by lowering the limit \n",
    "for determination of chromium from 0’5 to 0’1 percent; \n",
    "-.\n",
    "\n",
    "\n",
    "\n",
    "###question:\n",
    "2. When was this standard adopted?[/INST]\n",
    "\"\"\"\n",
    "\n",
    "generate_response(prompt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CASUAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     output_dir = \"KOKO\",\n",
    "#     max_steps = 100,\n",
    "#     per_device_train_batch_size = 4,\n",
    "#     warmup_steps = 0.03,\n",
    "#     logging_steps = 10,\n",
    "#     save_strategy = \"epoch\",\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=20,\n",
    "#     learning_rate=2e-4,\n",
    "#     lr_scheduler_type='constant',\n",
    "# )\n",
    "\n",
    "gradient_accumulation_steps = 4  # adjust this value based on your GPU memory\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"KOKO\",\n",
    "    max_steps = 100,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,  # add this line\n",
    "    warmup_steps = 0.03,\n",
    "    logging_steps = 10,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type='constant',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:342: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 256\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config = peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer = tokenizer,\n",
    "    formatting_func=create_prompt,\n",
    "    packing = True,\n",
    "    args = args,\n",
    "    train_dataset = selected_dataset_dict[\"train\"],\n",
    "    eval_dataset= selected_dataset_dict[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:50, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.947600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.184400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.092200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.4588971537351608, metrics={'train_runtime': 172.5, 'train_samples_per_second': 9.275, 'train_steps_per_second': 0.58, 'total_flos': 1.7410593543684096e+16, 'train_loss': 0.4588971537351608, 'epoch': 6.557377049180328})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt (no context)\n",
    "intstructions_string = f\"\"\" you are a textbot that helps in finding answers to questions in the research papers, blogs,pdf's or any text context.\n",
    ",make your answers more meaningful and short,end all responses with a signature with a newline in between\n",
    "\n",
    "-yourbot\n",
    "\n",
    "please answer the following question\n",
    "\"\"\"\n",
    "prompt_template = lambda question: f'''[INST] {intstructions_string} \\n{question} \\n[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  you are a textbot that helps in finding answers to questions in the research papers, blogs,pdf's or any text context.\n",
      ",make your answers more meaningful and short,end all responses with a signature with a newline in between\n",
      "\n",
      "-yourbot\n",
      "\n",
      "please answer the following question\n",
      " \n",
      "When was the mentioned standard adopted [date]? \n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "question = \"When was the mentioned standard adopted [date]?\"\n",
    "\n",
    "prompt = prompt_template(question)\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]  you are a textbot that helps in finding answers to questions in the research papers, blogs,pdf's or any text context.\n",
      ",make your answers more meaningful and short,end all responses with a signature with a newline in between\n",
      "\n",
      "-yourbot\n",
      "\n",
      "please answer the following question\n",
      " \n",
      "When was the mentioned standard adopted [date]? \n",
      "[/INST] ###answer:\n",
      "[year]</s>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt (no context)\n",
    "intstructions_string = f\"\"\" you are a textbot that helps in finding answers to questions in the research papers, blogs,pdf's or any text context.\n",
    ",make your answers more meaningful and short,end all responses with a signature after answer \"-yourbot\"\n",
    "\n",
    "please answer the following question\n",
    "\"\"\"\n",
    "prompt_template_w_context = lambda context, question: f'''[INST] {intstructions_string}\n",
    "\n",
    "{context}\n",
    "\n",
    "Please answer to the following question. Use the context above if it is helpful.\n",
    "\n",
    "{question}\n",
    "\n",
    "[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  you are a textbot that helps in finding answers to questions in the research papers, blogs,pdf's or any text context.\n",
      ",make your answers more meaningful and short,end all responses with a signature after answer \"-yourbot\"\n",
      "\n",
      "please answer the following question\n",
      "\n",
      "\n",
      "Context:\n",
      "IS : ‘228 ( Part 4 ) - 1987 \n",
      "Indian Standard \n",
      "METHODS FOR \n",
      "CHEMICAL ANALYSIS OF STEELS \n",
      "PART 4 DETERYlNATlON OF TOTAL CARBON \n",
      "BY GARVIMETRIC METHOD \n",
      "(FOR CARBON > O-1 PERCENT ) \n",
      "( Third Revision ) \n",
      "0. FOREWORD \n",
      "0.1 This Indian Standard ( Part 4 ) (Third R cvision ) was adopted I)y \n",
      "the Indian Standards Institution on 16 January 1987, afirr the draft \n",
      "finalized by the Methods of Chemical Analysis of Pcrrous Metals \n",
      "Sectional Committre had been approved by the Structural and Mct;~ls \n",
      "Division Council. \n",
      "0.2 IS : 228, which was issued as a tentative standard in 1952 and \n",
      "revised in 1959, covered the chemical analysis of pig iron, cast iron and \n",
      "plain carbon and low alloy steels.\n",
      "\n",
      "IS t \n",
      "Indian Standard 228 ( Part 5 ) - 1987 \n",
      "METHODS FOR \n",
      "CHEMICAL ANALYSIS OF STEELS \n",
      "PART 6 DETERMINATION OF NICKEL BY \n",
      "DIMETHYLGLYOXIME ( GRAVIMETRIC ) METHOD \n",
      "( FOR NICKEL > 0’1 PERCENT) \n",
      "( Third Revision ) \n",
      "0. FOREWORD \n",
      "0.1 This Indian Standard ( Part 5 ) ( Third Revision ) was adopted by \n",
      "the Indian Standards Institution on 16 January 1987, after the draft \n",
      "finalized by the Methods of Chemical Analysis of Ferrous Metals \n",
      "Sectional Committee had been approved by the Strtictural and Metals \n",
      "Division Council. \n",
      "0.2 IS : 228, which was issued as a tentative standard in 1952 and \n",
      "revised in 1959, covered the chemial analysis of pig iron, cast iron and \n",
      "plain carbon and low alloy steels.\n",
      "\n",
      "Accordingly, revision of IS : 228 was \n",
      "taken-up again and new series on methods of chemical analysis of \n",
      "steels including high alloy steels was published in various parts as \n",
      "IS : 228 ( Parts 1 to 13 ) ( see Appendix A ) covering separate method \n",
      "of analysis for each constituent in steels. However, IS : 228-1959* \n",
      "version has been retained for the analysis of pig iron and cast iron \n",
      "till a separate standard for analysis of pig iron and cast iron is \n",
      "published. \n",
      "0.2.1 This revision of IS : 228 (Part 6 )-1974t has been undertaken \n",
      "on the basis of experience gained during the implementation of the \n",
      "standard by the manufacturers and testing laboratories. \n",
      "0.3 In this revision, major modifications are: \n",
      "a) scope of the method has been modified by lowering the limit \n",
      "for determination of chromium from 0’5 to 0’1 percent; \n",
      "-.\n",
      "\n",
      "\n",
      "\n",
      "Please answer to the following question. Use the context above if it is helpful.\n",
      "\n",
      "When was the mentioned standard adopted [date]?\n",
      "\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template_w_context(context, question)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]  you are a textbot that helps in finding answers to questions in the research papers, blogs,pdf's or any text context.\n",
      ",make your answers more meaningful and short,end all responses with a signature after answer \"-yourbot\"\n",
      "\n",
      "please answer the following question\n",
      "\n",
      "\n",
      "Context:\n",
      "IS : ‘228 ( Part 4 ) - 1987 \n",
      "Indian Standard \n",
      "METHODS FOR \n",
      "CHEMICAL ANALYSIS OF STEELS \n",
      "PART 4 DETERYlNATlON OF TOTAL CARBON \n",
      "BY GARVIMETRIC METHOD \n",
      "(FOR CARBON > O-1 PERCENT ) \n",
      "( Third Revision ) \n",
      "0. FOREWORD \n",
      "0.1 This Indian Standard ( Part 4 ) (Third R cvision ) was adopted I)y \n",
      "the Indian Standards Institution on 16 January 1987, afirr the draft \n",
      "finalized by the Methods of Chemical Analysis of Pcrrous Metals \n",
      "Sectional Committre had been approved by the Structural and Mct;~ls \n",
      "Division Council. \n",
      "0.2 IS : 228, which was issued as a tentative standard in 1952 and \n",
      "revised in 1959, covered the chemical analysis of pig iron, cast iron and \n",
      "plain carbon and low alloy steels.\n",
      "\n",
      "IS t \n",
      "Indian Standard 228 ( Part 5 ) - 1987 \n",
      "METHODS FOR \n",
      "CHEMICAL ANALYSIS OF STEELS \n",
      "PART 6 DETERMINATION OF NICKEL BY \n",
      "DIMETHYLGLYOXIME ( GRAVIMETRIC ) METHOD \n",
      "( FOR NICKEL > 0’1 PERCENT) \n",
      "( Third Revision ) \n",
      "0. FOREWORD \n",
      "0.1 This Indian Standard ( Part 5 ) ( Third Revision ) was adopted by \n",
      "the Indian Standards Institution on 16 January 1987, after the draft \n",
      "finalized by the Methods of Chemical Analysis of Ferrous Metals \n",
      "Sectional Committee had been approved by the Strtictural and Metals \n",
      "Division Council. \n",
      "0.2 IS : 228, which was issued as a tentative standard in 1952 and \n",
      "revised in 1959, covered the chemial analysis of pig iron, cast iron and \n",
      "plain carbon and low alloy steels.\n",
      "\n",
      "Accordingly, revision of IS : 228 was \n",
      "taken-up again and new series on methods of chemical analysis of \n",
      "steels including high alloy steels was published in various parts as \n",
      "IS : 228 ( Parts 1 to 13 ) ( see Appendix A ) covering separate method \n",
      "of analysis for each constituent in steels. However, IS : 228-1959* \n",
      "version has been retained for the analysis of pig iron and cast iron \n",
      "till a separate standard for analysis of pig iron and cast iron is \n",
      "published. \n",
      "0.2.1 This revision of IS : 228 (Part 6 )-1974t has been undertaken \n",
      "on the basis of experience gained during the implementation of the \n",
      "standard by the manufacturers and testing laboratories. \n",
      "0.3 In this revision, major modifications are: \n",
      "a) scope of the method has been modified by lowering the limit \n",
      "for determination of chromium from 0’5 to 0’1 percent; \n",
      "-.\n",
      "\n",
      "\n",
      "\n",
      "Please answer to the following question. Use the context above if it is helpful.\n",
      "\n",
      "When was the mentioned standard adopted [date]?\n",
      "\n",
      "[/INST] The standard was adopted on January 16, 1987.</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"KOKO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface-hub -qU\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# login(token=\"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\", add_to_git_credential=True)\n",
    "# trainer.push_to_hub(\"Dobby/KOKO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/backup/viswaz/Project_K/.venv/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "merged_model = model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs,\n",
    "                                 max_new_tokens=150,\n",
    "                                 do_sample=True,\n",
    "                                 pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "  # Split the generated text into prompt and answer\n",
    "  split_index = decoded_output[0].find('answer:')\n",
    "  if split_index == -1:\n",
    "    split_index = decoded_output[0].find('Answer:')\n",
    "  decoded_output = [s.strip(\"'\") for s in decoded_output]\n",
    "  decoded_output = [s.replace('</s>', '') for s in decoded_output]\n",
    "  answer = decoded_output[0][split_index+len('answer:'):].strip()\n",
    "  answer = answer.replace('[INST]'or '[ANS]' , '').replace('[/INST]' or '[/ANS]' or '[//INST]', '')\n",
    "  answer = re.sub(r'\\[.*?\\]', '', answer)\n",
    "\n",
    "  # Stop decoding when it encounters a ### token\n",
    "  stop_index = answer.find('###')\n",
    "  if stop_index != -1:\n",
    "    answer = answer[:stop_index].strip()\n",
    "\n",
    "  return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This Indian Standard (Part 5) (Third Revision) was adopted by the Indian Standards Institution on 16 January 1987.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "[INST]\n",
    "###Instruction:\n",
    "below context is from 228_4.pdf, answer the following questions based on the context given\n",
    "\n",
    "\n",
    "###pdf_filename:\n",
    "228_4.pdf\n",
    "\n",
    "###context:\n",
    "Context:\n",
    "IS : ‘228 ( Part 4 ) - 1987 \n",
    "Indian Standard \n",
    "METHODS FOR \n",
    "CHEMICAL ANALYSIS OF STEELS \n",
    "PART 4 DETERYlNATlON OF TOTAL CARBON \n",
    "BY GARVIMETRIC METHOD \n",
    "(FOR CARBON > O-1 PERCENT ) \n",
    "( Third Revision ) \n",
    "0. FOREWORD \n",
    "0.1 This Indian Standard ( Part 4 ) (Third R cvision ) was adopted I)y \n",
    "the Indian Standards Institution on 16 January 1987, afirr the draft \n",
    "finalized by the Methods of Chemical Analysis of Pcrrous Metals \n",
    "Sectional Committre had been approved by the Structural and Mct;~ls \n",
    "Division Council. \n",
    "0.2 IS : 228, which was issued as a tentative standard in 1952 and \n",
    "revised in 1959, covered the chemical analysis of pig iron, cast iron and \n",
    "plain carbon and low alloy steels.\n",
    "\n",
    "IS t \n",
    "Indian Standard 228 ( Part 5 ) - 1987 \n",
    "METHODS FOR \n",
    "CHEMICAL ANALYSIS OF STEELS \n",
    "PART 6 DETERMINATION OF NICKEL BY \n",
    "DIMETHYLGLYOXIME ( GRAVIMETRIC ) METHOD \n",
    "( FOR NICKEL > 0’1 PERCENT) \n",
    "( Third Revision ) \n",
    "0. FOREWORD \n",
    "0.1 This Indian Standard ( Part 5 ) ( Third Revision ) was adopted by \n",
    "the Indian Standards Institution on 16 January 1987, after the draft \n",
    "finalized by the Methods of Chemical Analysis of Ferrous Metals \n",
    "Sectional Committee had been approved by the Strtictural and Metals \n",
    "Division Council. \n",
    "0.2 IS : 228, which was issued as a tentative standard in 1952 and \n",
    "revised in 1959, covered the chemial analysis of pig iron, cast iron and \n",
    "plain carbon and low alloy steels.\n",
    "\n",
    "Accordingly, revision of IS : 228 was \n",
    "taken-up again and new series on methods of chemical analysis of \n",
    "steels including high alloy steels was published in various parts as \n",
    "IS : 228 ( Parts 1 to 13 ) ( see Appendix A ) covering separate method \n",
    "of analysis for each constituent in steels. However, IS : 228-1959* \n",
    "version has been retained for the analysis of pig iron and cast iron \n",
    "till a separate standard for analysis of pig iron and cast iron is \n",
    "published. \n",
    "0.2.1 This revision of IS : 228 (Part 6 )-1974t has been undertaken \n",
    "on the basis of experience gained during the implementation of the \n",
    "standard by the manufacturers and testing laboratories. \n",
    "0.3 In this revision, major modifications are: \n",
    "a) scope of the method has been modified by lowering the limit \n",
    "for determination of chromium from 0’5 to 0’1 percent; \n",
    "-.\n",
    "\n",
    "\n",
    "\n",
    "###question:\n",
    "2. When was this standard adopted?[/INST]\n",
    "\"\"\"\n",
    "\n",
    "generate_response(prompt, merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
