{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install datasets libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dataset = load_dataset(\"nvidia/ChatQA-Training-Data\", \"drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check if the dataset is loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets check the contents of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in loaded_dataset:\n",
    "  print(i)\n",
    "  count += 1\n",
    "  if count == 5:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### oops its not a dataset !! then what ? why are we getting only train ? only one example ? where are other examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### it is a dictionary of datasets check nvidia/ChatQA-Training-Data on hugging face we downloaded a part of it which is under the column drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in loaded_dataset[\"train\"]:\n",
    "  print(i)\n",
    "  count += 1\n",
    "  if count == 5:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dataset[\"train\"][\"messages\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# !pip install pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# # Load the Excel file\n",
    "# excel_file = pd.ExcelFile('Internship_AI_application in civil(AutoRecovered).xlsx')\n",
    "# output_folder = 'excels'\n",
    "\n",
    "# # Create the output folder if it doesn't exist\n",
    "# if not os.path.exists(output_folder):\n",
    "#     os.makedirs(output_folder)\n",
    "\n",
    "# # Load the Excel file\n",
    "# excel_file = pd.ExcelFile(excel_file)\n",
    "\n",
    "# # Loop through each sheet and save it as a separate Excel file in the output folder\n",
    "# for sheet_name in excel_file.sheet_names:\n",
    "#     df = excel_file.parse(sheet_name)\n",
    "#     output_file_path = os.path.join(output_folder, f'{sheet_name}.xlsx')\n",
    "#     df.to_excel(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "\n",
    "# # Specify the path to the folder containing the CSV files\n",
    "# folder_path = '/mnt/data1/backup/viswaz/Project_K/QNA'  # Current directory\n",
    "\n",
    "# # Get a list of all CSV files in the folder\n",
    "# csv_files = glob.glob(os.path.join(folder_path, '*.xlsx'))\n",
    "\n",
    "# # Delete each CSV file\n",
    "# for csv_file in csv_files:\n",
    "#     os.remove(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Define the directory path\n",
    "# directory = \"/mnt/data1/backup/viswaz/Project_K/excels/\"\n",
    "\n",
    "# # Iterate over each file in the directory\n",
    "# for filename in os.listdir(directory):\n",
    "#     # Check if the file name starts with 'IS_' and contains a number after it\n",
    "#     if filename.startswith(\"IS_\"):\n",
    "#         # Extract the number after 'IS_'\n",
    "#         parts = filename.split(\"_\")\n",
    "#         if len(parts) > 1:\n",
    "#             number = parts[1]\n",
    "#             # Rename the file with the extracted number\n",
    "#             os.rename(os.path.join(directory, filename), os.path.join(directory, number + \".xlsx\"))\n",
    "#             print(f\"Renamed {filename} to {number}.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the directory path containing Excel files\n",
    "# directory = \"/mnt/data1/backup/viswaz/Project_K/QNA\"\n",
    "\n",
    "# # Iterate over each file in the directory\n",
    "# for filename in os.listdir(directory):\n",
    "#     # Check if the file is an Excel file\n",
    "#     if filename.endswith(\".xlsx\"):\n",
    "#         # Construct the full file path\n",
    "#         filepath = os.path.join(directory, filename)\n",
    "        \n",
    "#         # Read the Excel file into a DataFrame\n",
    "#         df = pd.read_excel(filepath)\n",
    "        \n",
    "#         # Construct the output CSV file path\n",
    "#         csv_filepath = os.path.splitext(filepath)[0] + \".csv\"\n",
    "        \n",
    "#         # Write the DataFrame to a CSV file\n",
    "#         df.to_csv(csv_filepath, index=False)\n",
    "#         print(f\"Converted {filename} to {csv_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the directory path containing CSV files\n",
    "# directory = \"QNA\"\n",
    "\n",
    "# # Iterate over each file in the directory\n",
    "# for filename in os.listdir(directory):\n",
    "#     # Check if the file is a CSV file\n",
    "#     if filename.endswith(\".csv\"):\n",
    "#         # Construct the full file path\n",
    "#         filepath = os.path.join(directory, filename)\n",
    "#         print(filepath)\n",
    "#         # Read the CSV file into a DataFrame\n",
    "#         df = pd.read_csv(filepath)\n",
    "        \n",
    "#         # Check if the DataFrame has three columns\n",
    "#         if len(df.columns) == 3:\n",
    "#             # Remove the first column\n",
    "#             df = df.iloc[:, 1:]\n",
    "            \n",
    "#             # Write the modified DataFrame back to the CSV file\n",
    "#             df.to_csv(filepath, index=False)\n",
    "#             print(f\"Removed the first column from {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the directory path containing CSV files\n",
    "# directory = \"QNA\"\n",
    "\n",
    "# # Iterate over each file in the directory\n",
    "# for filename in os.listdir(directory):\n",
    "#     # Check if the file is a CSV file\n",
    "#     if filename.endswith(\".csv\"):\n",
    "#         # Construct the full file path\n",
    "#         filepath = os.path.join(directory, filename)\n",
    "        \n",
    "#         # Read the CSV file into a DataFrame\n",
    "#         df = pd.read_csv(filepath)\n",
    "        \n",
    "#         # Check if the DataFrame has exactly two columns\n",
    "#         if len(df.columns) == 2:\n",
    "#             # Rename the columns to 'Question' and 'Answer'\n",
    "#             df.columns = ['Question', 'Answer']\n",
    "            \n",
    "#             # Write the modified DataFrame back to the CSV file\n",
    "#             df.to_csv(filepath, index=False)\n",
    "#             print(f\"Renamed columns in {filename} to 'Question' and 'Answer'\")\n",
    "#         else:\n",
    "#             print(f\"Error: {filename} does not have exactly two columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as pdf_file_obj:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
    "        text = ''\n",
    "        for page_obj in pdf_reader.pages:\n",
    "            text += page_obj.extract_text()\n",
    "    return text\n",
    "\n",
    "def prepare_qa_data(qa_directory, pdf_directory):\n",
    "    qa_data = []\n",
    "    for filename in os.listdir(qa_directory):\n",
    "        # Check if the file is a CSV\n",
    "        if filename.endswith('.csv'):\n",
    "            # Construct the full file path\n",
    "            csv_file_path = os.path.join(qa_directory, filename)\n",
    "            # Extract the base name of the file (without extension)\n",
    "            base_filename = os.path.splitext(filename)[0]\n",
    "            # Construct the full file path of the corresponding PDF\n",
    "            pdf_file_path = os.path.join(pdf_directory, f'{base_filename}.pdf')\n",
    "            # Read the CSV file\n",
    "            qa_df = pd.read_csv(csv_file_path)\n",
    "            # Extract the text from the PDF file\n",
    "            text = extract_text_from_pdf(pdf_file_path)\n",
    "            # Preprocess the text\n",
    "            # preprocessed_text = preprocess_text(text)\n",
    "            # Iterate over the rows in the CSV file\n",
    "            for _, row in qa_df.iterrows():\n",
    "                question = row['Question']\n",
    "                answer = row['Answer']\n",
    "                # Include the filename of the corresponding PDF\n",
    "                pdf_filename = base_filename + '.pdf'\n",
    "                qa_data.append({'pdf_filename': pdf_filename, 'context': text, 'question': question, 'answer': answer })\n",
    "    return qa_data\n",
    "\n",
    "qa_directory = 'QNA'\n",
    "pdf_directory = 'pdf'\n",
    "qa_data = prepare_qa_data(qa_directory, pdf_directory)\n",
    "print(qa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_data[100]\n",
    "print(len(qa_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_as_jsonl(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in data:\n",
    "            f.write(json.dumps(i) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_directory = 'QNA'\n",
    "pdf_directory = 'pdf'\n",
    "qa_data = prepare_qa_data(qa_directory, pdf_directory)\n",
    "\n",
    "train_size = int(len(qa_data) * 0.8)\n",
    "train_data = qa_data[:train_size]\n",
    "test_data = qa_data[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_jsonl(train_data, \"train.jsonl\")\n",
    "save_as_jsonl(test_data, \"test.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_files = {\"train\":\"train.jsonl\", \"test\":\"test.jsonl\"}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\"KOKO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
